{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "# nltk.download('punkt')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import sys, io\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.proxy import *\n",
    "from joblib import load, dump \n",
    "import pandas as pd # Pandas documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html\n",
    "import enchant\n",
    "import string\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "chrome_options.binary_location = '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications = [\n",
    "    'facebook',\n",
    "    'snapchat',\n",
    "    'tiktok',\n",
    "    'reddit',\n",
    "    'spotify'\n",
    "]\n",
    "\n",
    "applications_to_urls = {\n",
    "    applications[0] :'https://play.google.com/store/apps/details?id=com.facebook.katana&hl=en_US',\n",
    "    applications[1]:'https://play.google.com/store/apps/details?id=com.snapchat.android&hl=en_US',\n",
    "    applications[2]:'https://play.google.com/store/apps/details?id=com.zhiliaoapp.musically&hl=en_US',\n",
    "    applications[3]:'https://play.google.com/store/apps/details?id=com.reddit.frontpage&hl=en_US',\n",
    "    applications[4]:'https://play.google.com/store/apps/details?id=com.spotify.music'\n",
    "}\n",
    "\n",
    "urls_to_applications = {}\n",
    "for (k, v) in applications_to_urls.items():\n",
    "    urls_to_applications[v] = k\n",
    "    \n",
    "applications_to_save_files = {}\n",
    "for (k,_) in applications_to_urls.items():\n",
    "    applications_to_save_files[k] = k + '_reviews'\n",
    "\n",
    "applications_to_reviews_by_sentence = {}\n",
    "for (k,_) in applications_to_urls.items():\n",
    "    applications_to_reviews_by_sentence[k] = applications_to_save_files[k] + '_by_sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktok_reviews = pd.read_json('tiktok_reviews.json') \n",
    "facebook_reviews = pd.read_json('facebook_reviews.json') \n",
    "snapchat_reviews = pd.read_json('snapchat_reviews.json') \n",
    "reddit_reviews = pd.read_json('reddit_reviews.json')\n",
    "spotify_reviews = pd.read_json('spotify_reviews.json')\n",
    "review_json_files = ['tiktok_reviews.json', 'facebook_reviews.json', 'snapchat_reviews.json', 'reddit_reviews.json', 'spotify_reviews.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_review_df = pd.DataFrame(columns=['review_body', 'found_helpful', 'application'])\n",
    "# for json_file in review_json_files:\n",
    "#     application_review = pd.read_json(json_file)\n",
    "#     application = json_file.split('_')[0]\n",
    "#     for i in range(len(application_review)):\n",
    "#         page_expansion = BeautifulSoup(application_review['review_body'][i], 'html.parser')\n",
    "#         full_review = page_expansion.find('span', {'jsname':'fbQN7e'})\n",
    "#         page_expansion = BeautifulSoup(application_review['found_helpful'][i], 'html.parser')\n",
    "#         helpful_rating = page_expansion.find('div', class_='jUL89d')\n",
    "#         helpful_rating = int(helpful_rating.text) if len(helpful_rating) != None else 0\n",
    "#         if helpful_rating > 200:\n",
    "#             json_review_df = json_review_df.append({'review_body': full_review.text, 'found_helpful': helpful_rating, 'application': application}, ignore_index=True)\n",
    "# print(len(json_review_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split by sentence\n",
    "# json_reviews_by_sent = pd.DataFrame(columns=['review_num', 'review_body', 'application', 'classification'])\n",
    "# for i in range(len(json_review_df)):\n",
    "#     review = json_review_df['review_body'][i]\n",
    "#     sentences = nltk.tokenize.sent_tokenize(review)\n",
    "#     for sentence in sentences:\n",
    "#         json_reviews_by_sent = json_reviews_by_sent.append({'review_num': i, 'review_body': sentence, 'application': json_review_df['application'][i],'classification': None},\n",
    "#                                                           ignore_index=True) \n",
    "# print(len(json_reviews_by_sent))\n",
    "# json_reviews_by_sent = json_reviews_by_sent.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # label reviews as informative or uninformative\n",
    "# # DO NOT RUN THIS IF DATA IS LABELED ALREADY\n",
    "# for i in range(1000):\n",
    "#     # print(\"======================\")\n",
    "#     if json_reviews_by_sent['classification'][i] == None: \n",
    "#         print('Review ' + str(i))\n",
    "#         print(json_reviews_by_sent['review_body'][i])\n",
    "#         classification = input('Informative or non-informative? ') \n",
    "#         if classification == 'n' or classification == 'i':\n",
    "#             classification = 'non-informative' if classification == 'n' else 'informative'\n",
    "#         if classification == 'v':\n",
    "#             classification = 'vague'\n",
    "#         json_reviews_by_sent['classification'][i] = classification\n",
    "#     dump(json_reviews_by_sent, 'json_review_dataframe_by_sent', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(sentence):\n",
    "    no_punc = [c for c in sentence if c not in string.punctuation]\n",
    "    no_punc = ''.join(no_punc)\n",
    "    no_stopwords = [w.lower() for w in no_punc.split() if (w not in stopwords_set) and (len(re.search('^\\s*[0-9]*', w)[0]) == 0)]    \n",
    "    stemmed_words = [ps.stem(w) for w in no_stopwords]\n",
    "    return stemmed_words\n",
    "\n",
    "def clean_review_len(sentence):\n",
    "    return len(clean_review(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "i = 100\n",
    "\n",
    "def get_sentiment(s):\n",
    "    vs = analyzer.polarity_scores(s)\n",
    "    if vs['compound'] >= 0.05:\n",
    "        return 2\n",
    "    elif vs['compound'] <= -0.05:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "def get_strict_sentiment(s):\n",
    "    vs = analyzer.polarity_scores(s)\n",
    "    if vs['compound'] >= 0.0:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_reviews_by_sent = load('json_review_dataframe_by_sent')\n",
    "json_reviews_by_sent['sentiment'] = json_reviews_by_sent['review_body'].apply(get_sentiment)\n",
    "json_word_freq_table = create_word_freq_table(json_reviews_by_sent)\n",
    "json_word_freq_table['_sentiment'] = json_reviews_by_sent['sentiment']\n",
    "json_word_freq_table['_classification'] = json_reviews_by_sent['classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num informative:  361\n",
      "Num non-informative:  442\n",
      "Num vague:  131\n"
     ]
    }
   ],
   "source": [
    "# json_reviews_by_sent['clean_review_body'] = json_reviews_by_sent['review_body'].apply(clean_review)\n",
    "# json_reviews_by_sent['review_body'] = json_reviews_by_sent['review_body'].apply(fix_tiktok)\n",
    "# json_reviews_by_sent = load('json_review_dataframe')\n",
    "print('Num informative: ', len(json_reviews_by_sent[(json_reviews_by_sent['classification'] == 'informative')]))\n",
    "print('Num non-informative: ', len(json_reviews_by_sent[(json_reviews_by_sent['classification'] == 'non-informative')]))\n",
    "print('Num vague: ', len(json_reviews_by_sent[(json_reviews_by_sent['classification'] == 'vague')]))\n",
    "# perform whatever manipulations you want on the json_reviews_by_sent dataframe\n",
    "non_informative_df = json_reviews_by_sent[(json_reviews_by_sent['classification'] == 'non-informative')]\n",
    "informative_df = json_reviews_by_sent[(json_reviews_by_sent['classification'] == 'informative')]\n",
    "classification_df = informative_df.append(non_informative_df[:361], ignore_index=True)\n",
    "\n",
    "# drop irrelevant columns\n",
    "classification_df.drop(['review_num', 'application', 'length'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# With words_freq_table\n",
    "non_informative_df = json_word_freq_table[(json_word_freq_table['_classification'] == 'non-informative')]\n",
    "informative_df = json_word_freq_table[(json_word_freq_table['_classification'] == 'informative')]\n",
    "classification_df = informative_df.append(non_informative_df[:361], ignore_index=True)\n",
    "# classification_df = classification_df.append(json_word_freq_table[(json_word_freq_table['_classification'] == 'vague')], ignore_index=True)\n",
    "\n",
    "# trying 3 categories\n",
    "# classification_df = json_word_freq_table[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# msg_train, msg_test, label_train, label_test = train_test_split(classification_df.drop('classification', axis=1), classification_df['classification'], test_size=0.3, random_state=42)\n",
    "msg_train, msg_test, label_train, label_test = train_test_split(classification_df.drop('_classification', axis=1), classification_df['_classification'], test_size=0.3, random_state=42)\n",
    "param_grid = {'classifier__C': [0.1,1, 10, 100, 1000], 'classifier__gamma': [1,0.1,0.01,0.001,0.0001], 'classifier__kernel': ['rbf']} \n",
    "param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    informative       0.86      0.76      0.81       129\n",
      "non-informative       0.70      0.82      0.75        88\n",
      "\n",
      "       accuracy                           0.78       217\n",
      "      macro avg       0.78      0.79      0.78       217\n",
      "   weighted avg       0.79      0.78      0.79       217\n",
      "\n",
      "Random Forest Classifier\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    informative       0.70      0.93      0.80        86\n",
      "non-informative       0.94      0.74      0.83       131\n",
      "\n",
      "       accuracy                           0.82       217\n",
      "      macro avg       0.82      0.84      0.81       217\n",
      "   weighted avg       0.85      0.82      0.82       217\n",
      "\n",
      "SVM\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    informative       0.79      0.94      0.86        96\n",
      "non-informative       0.94      0.80      0.87       121\n",
      "\n",
      "       accuracy                           0.86       217\n",
      "      macro avg       0.87      0.87      0.86       217\n",
      "   weighted avg       0.87      0.86      0.86       217\n",
      "\n",
      "Bernoulli\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    informative       0.67      0.94      0.78        81\n",
      "non-informative       0.95      0.72      0.82       136\n",
      "\n",
      "       accuracy                           0.80       217\n",
      "      macro avg       0.81      0.83      0.80       217\n",
      "   weighted avg       0.85      0.80      0.80       217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pipeline1 = Pipeline([\n",
    "#     ('bow', CountVectorizer(analyzer=clean_review)),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('classifier', MultinomialNB())\n",
    "# ])\n",
    "\n",
    "# pipeline2 = Pipeline([\n",
    "#     ('bow', CountVectorizer(analyzer=clean_review)),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('classifier', BernoulliNB())\n",
    "# ])\n",
    "\n",
    "\n",
    "# pipeline3 = Pipeline([\n",
    "#     ('bow', CountVectorizer(analyzer=clean_review)),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('classifier', SVC(C=10, gamma=1, kernel='rbf'))\n",
    "# ])\n",
    "\n",
    "# # grid = GridSearchCV(pipeline3, param_grid, refit=True, verbose=0, cv=10)\n",
    "# # 0.7557142857142857\n",
    "# # {'classifier__C': 10, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}\n",
    "\n",
    "# pipeline4 = Pipeline([\n",
    "#     ('bow', CountVectorizer(analyzer=clean_review)),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('classifier', RandomForestClassifier(n_estimators=200))\n",
    "# ])\n",
    "\n",
    "############################################################################################################################################\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(msg_train, label_train)\n",
    "predictions1 = mnb.predict(msg_test)\n",
    "print('Multinomial')\n",
    "print(classification_report(predictions1, label_test))\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(msg_train, label_train)\n",
    "predictions2 = rfc.predict(msg_test)\n",
    "print('Random Forest Classifier')\n",
    "print(classification_report(predictions2, label_test))\n",
    "\n",
    "# grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=0, cv=10)\n",
    "# grid.fit(msg_train, label_train)\n",
    "# predictions3 = grid.predict(msg_test)\n",
    "# print('Support Vector Machine')\n",
    "# print(grid.best_params_)\n",
    "# print(grid.best_estimator_)\n",
    "# print(classification_report(predictions3, label_test))\n",
    "\n",
    "\n",
    "svm = SVC(C = 10, gamma= 0.01)\n",
    "svm.fit(msg_train, label_train)\n",
    "predictions3 = svm.predict(msg_test)\n",
    "print('SVM')\n",
    "print(classification_report(predictions3, label_test))\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(msg_train, label_train)\n",
    "predictions4 = bnb.predict(msg_test)\n",
    "print('Bernoulli')\n",
    "print(classification_report(predictions4, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline1.fit(msg_train, label_train)\n",
    "# pipeline2.fit(msg_train, label_train)\n",
    "# pipeline3.fit(msg_train, label_train)\n",
    "# pipeline4.fit(msg_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76 (+/- 0.11)\n",
      "Accuracy: 0.79 (+/- 0.14)\n",
      "Accuracy: 0.83 (+/- 0.12)\n",
      "Accuracy: 0.79 (+/- 0.14)\n"
     ]
    }
   ],
   "source": [
    "scores1 = cross_val_score(mnb, msg_train, label_train, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores1.mean(), scores1.std() * 2))\n",
    "scores2 = cross_val_score(rfc, msg_train, label_train, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores2.mean(), scores2.std() * 2))\n",
    "scores3 = cross_val_score(svm, msg_train, label_train, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores3.mean(), scores3.std() * 2))\n",
    "scores4 = cross_val_score(bnb, msg_train, label_train, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores4.mean(), scores4.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = mnb.predict(msg_test)\n",
    "predictions2 = rfc.predict(msg_test)\n",
    "predictions3 = svm.predict(msg_test)\n",
    "predictions4 = bnb.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_reviews_by_sent['length'] = json_reviews_by_sent['review_body'].apply(clean_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_reviews_by_sent.hist(column='length', by='classification', bins=50,figsize=(12,4))\n",
    "# json_reviews_by_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    informative       0.86      0.76      0.81       129\n",
      "non-informative       0.70      0.82      0.75        88\n",
      "\n",
      "       accuracy                           0.78       217\n",
      "      macro avg       0.78      0.79      0.78       217\n",
      "   weighted avg       0.79      0.78      0.79       217\n",
      "\n",
      "RFC\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    informative       0.70      0.93      0.80        86\n",
      "non-informative       0.94      0.74      0.83       131\n",
      "\n",
      "       accuracy                           0.82       217\n",
      "      macro avg       0.82      0.84      0.81       217\n",
      "   weighted avg       0.85      0.82      0.82       217\n",
      "\n",
      "SVM\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    informative       0.79      0.94      0.86        96\n",
      "non-informative       0.94      0.80      0.87       121\n",
      "\n",
      "       accuracy                           0.86       217\n",
      "      macro avg       0.87      0.87      0.86       217\n",
      "   weighted avg       0.87      0.86      0.86       217\n",
      "\n",
      "Bernoulli\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    informative       0.67      0.94      0.78        81\n",
      "non-informative       0.95      0.72      0.82       136\n",
      "\n",
      "       accuracy                           0.80       217\n",
      "      macro avg       0.81      0.83      0.80       217\n",
      "   weighted avg       0.85      0.80      0.80       217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Multinomial')\n",
    "print(classification_report(predictions1, label_test))\n",
    "print('RFC')\n",
    "print(classification_report(predictions2, label_test))\n",
    "print('SVM')\n",
    "print(classification_report(predictions3, label_test))\n",
    "print('Bernoulli')\n",
    "print(classification_report(predictions4, label_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['json_review_dataframe_by_sent']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = svm.predict(json_word_freq_table[1001:].drop('_classification', axis=1).fillna(0))\n",
    "for i in range(len(pred)):\n",
    "    if json_reviews_by_sent['classification'][1001+i] == None:\n",
    "        json_reviews_by_sent['classification'][1001+i] = pred[i]\n",
    "dump(json_reviews_by_sent, 'json_review_dataframe_by_sent', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_model']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(svm, 'best_model', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_df = json_reviews_by_sent[(json_reviews_by_sent['classification'] == 'informative')]\n",
    "# ni_df = json_reviews_by_sent[(json_reviews_by_sent['classification'] == 'non-informative')]\n",
    "# v_df = json_reviews_by_sent[(json_reviews_by_sent['classification'] == 'vague')]\n",
    "# print(i_df['review_body'][1])\n",
    "# print(v_df['review_body'][50])\n",
    "# print(ni_df['review_body'][4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_freq_table(input_df):\n",
    "    bow_transformer = CountVectorizer(analyzer=clean_review).fit(input_df['review_body'])\n",
    "    transformed_input = bow_transformer.transform(input_df['review_body'])\n",
    "    count_vect_df = pd.DataFrame(transformed_input.todense(), columns=bow_transformer.get_feature_names())\n",
    "    return count_vect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeling_df = json_reviews_by_sent[json_reviews_by_sent['classification'] == 'informative']\n",
    "json_bow = create_word_freq_table(topic_modeling_df)\n",
    "json_bow\n",
    "tdm = json_bow.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sometimes the radius selection tool works, sometimes it shows a 200km radius even though you're trying to select 10km.\n",
      "And are stuck with it, but we know the interface is laughable right?\n",
      "Like of you only put 6 songs on a playlist, it adds suggested songs and I hate that so much.\n",
      "Can't upload video over 1080p.\n",
      "When you click them; it takes you to the right place but wont let you see what the notification was for.\n",
      "I normally have no problems at all with Facebook an I would have given 5 stars but over the last 2 wks it's been a complete nightmare..home page is constantly stuck on the same feed notifications are no longer working ,see 1st is the same I've not had one at the beginning of the home screen ...I've try'd contacting you on several occasions to see I'd theres any way it can be sorted lol I'm not even getting a reply from that ..please please please for my sanity help\n",
      "Can't push several different buttons at time: admin button off and on, clicking on profiles, saving and glitches, worked on my iPhone just fine.\n",
      "I have also noticed that double tapping someone to open the camera it sometimes opens the camera for a few seconds before bringing the chat up instead, so you have to go back out of the chat to get to the camera.\n",
      "Too many ads, all similar services are advertised to me even after removing things from my information.\n",
      "I also get that notification at the top right over messages on the fb app and I'll check, there would be nothing there.\n",
      "I can not thank you enough for showing me people that don't even live in my country when I'm trying to search someone's name in my area...... Clown show.\n",
      "I can't like ANY pages, only follow.\n",
      "It's bad enough to have updates just about every month, but the pop ups on my home and lock screens must stop!!\n",
      "The updates are good but some features are only for apple.\n",
      "When I open it, it has the white screen with their symbol and a blue dot that just keeps bouncing through its 5 dots.\n",
      "Even with incredible internet connection, it will often not send snaps or messages.\n",
      "They broke night mode so you can no longer have it on all the time.The app no longer keeps track of where you were when you last switched to a different application.\n",
      "The automatic deleting of messages some time after they're opened is perhaps the most confusing choice.\n",
      "I have a play list and it constantly plays songs that swear in every sentence.\n",
      "I gave this app 3 stars bc lately it's been saying I've opened snaps or messages when I didn't.\n",
      "3: I'll sometimes just open facebook and go to notifications and it's just a blank screen, nothing there at all and I'll have to switch back and forth between News Feed and Notifications for it to bring up the Notis.\n",
      "Should have integrated the chat app with social media app.\n",
      "It isn't like this when I'm using the desktop so I guess the problem is really on the app.\n",
      "Because sometimes it's all the way down and it's hard scrolling down to thousands of pictures and videos.\n",
      "Sometimes duplicated notifications.\n",
      "I was in college for programming, and it being unable to load this long is completely unnecessary, and unacceptable.\n",
      "I cannot hide stories from the discover section: Once I click hide, it still pops up after I refresh it, even after I report it, it still shows up.\n",
      "It's ridiculous how much this app affects the overall performance of my phone.\n",
      "You can't go through the tree of discovering artists after your first login, and sometimes that doesn't even come up the first time (happened with my dad).\n",
      "I don't get the new notifications right away despite my settings to show all.\n",
      "And it does an extraordinary job of reccommending new music that suits my tastes.\n",
      "The update today won't let anything load.\n",
      "Now a lot of times when I get a Snapchat, I open the app, go to all my snaps, and it says I already opened the snap.\n",
      "The lite version works great but then I can't view polls so that's a no go as well.\n",
      "The GAMES LAG when you're trying to answer regardless of which game!\n",
      "I, personally use my phone number for business only and use Facebook messenger for both text chatting and high quality voice and video calls\n",
      "Every time I look at a page that I follow it constantly refreshes the content of the page and I'm unable to see any recent posts made by the page.\n",
      "Unfortunately my news feed seems to have broken as I seem to getting the same posts no matter what that can be days old.\n",
      "It keeps notifying you even after you click on the notification.. ALSO, the Messenger notification link in the upper right hand of the app keeps alerting me of a message when I have no current messages!\n",
      "It has been a week since I sent the required documents to prove my identity and I still am not able to log in to my account.\n",
      "I'm sorry if I'm only a stupid b who doesn't know how to fix this, but I want my play button back(instead of shuffle play).\n",
      "Left and right texts on chat screen\n",
      "I can no longer use them to pause or play a podcast, but can use them to move between episodes within a podcast.\n",
      "Version for Android users isn't as good as iOS users.\n",
      "As for fb dating- it never has/will let me view my own profile so I cant do anything except like someone and message them.\n",
      "It just won't send.\n",
      "At LEAST half of the content I see is advertisements or \"sponsored\" posts.\n",
      "I wish there was a separate app to discover communities of interest, join them and help other people.\n",
      "It's OK, there should be a setting in the editing stage where you can chose where to go instead at starting at the start every single time and needing to wait the entire video to edit, also it's annoying how for the audios, it goes into one big pile so they get lost and I can't find the one I want to use.\n",
      "l-l literally it kicks me out when I just scrolled less then 10 videos it kicks me out all the time it's so annoying I can't do this it's just like you can only scroll like 10 videos and then you had to restart like literally so annoying Tik Tok what's what's with it?\n",
      "I've been using this for a very long time and i didn't experience any kind of problem but i hope that snapchat should offer dark mode ..it would be very helpful and attractive for the users.\n",
      "I clear the cache and data for the app and the problem keeps coming back, the same posts, in the same order and that's after several hours have gone by and several tries at refreshing the feed with no change in results.\n",
      "This is a realy good app and as other apps try to copy the content (basicly what your app is) but you will always be on top :) tiktok is a amazing app to show think what your proud of, acheved, and want to show with the world right now i have a acount and im almost to 30 followers :D this is a rly good up just wish musicly/the name where still here but name tiktok gives it meaning in some how well.\n",
      "also whenever I post something on my wall I have to sign out then sign back in and then refresh about four or five times before it appears.\n",
      "Also, I hate it when I'm making a playlist and they add songs to the end of it that I don't like.\n",
      "When I have it hooked up to my car through Bluetooth, it frequently stops the music.\n",
      "I can see all my private vedios but out of 197 vedios I have posted only 1st 2 is shown in my page.\n",
      "Sometimes my news feed shows same posts from my friends list from a few days before.\n",
      "Since the last update, every single time I open the app, I get 3 to 5 downloads popping up in download manager and it says \"Feature on Facebook\" Its very annoying and unnecessary and needs to stop.\n",
      "The app has wiped out my previous list and thinks there aren't any.\n",
      "But if I go out of the app for a few seconds and then back in, the groups are greyed out and it just shows a continuous loop updating, but nothing happens.\n",
      "Sometimes stories refreshes every 2 sec.\n",
      "End up losing my spot because even if im on a post, the screen blacks out until a back out and the app refreshes.\n",
      "Only reason I have used it so far is because everyone else does and I'm forced to use it too to get updates from the group chats.\n",
      "Why can't others see my posts in the quality that I captured them in?\n",
      "Slow even with 4g and and a 8GB ram phone.\n",
      "But since two months now the app isn't responding very well.\n",
      "I think this app is well laid out but sometimes interacting with posts is hard as there are too many images used.\n",
      "Ads keep pushing in the background while video is playing.\n",
      "I can't see anything and the audio still works.\n",
      "I tried contacting you in every ways possible but I didn't get any response, this made me very disappointed.\n",
      "Also, doesn't load older \"Saved in chat\" pics and videos.\n",
      "If I have it running in the background and playing music, when I reopen it, the song will skip a few seconds ahead.\n",
      "The FB App can't seem to find, and there's no function to browse for all the stored galleries of images in the phone's internal or external storage.\n",
      "Every time I log in or open the app, the top part of my screen is cut off by the selection bar on the top.\n",
      "It was disabled after less then 5 minutes, (11 minutes of a livestreamed massacre somehow slipped through their security algorithms though) I have since sent all required proof of identity multiple times, not a single response, or any resolution.\n",
      "Oh and I can no longer edit my posts anymore once they have been posted, nor can I delete them.\n",
      "I love to mess around with streaks so can you please try and do an update where on android you can zoom in on a gify more because it makes it extremely hard to come up with creative ideas for streaks.\n",
      "I couldnt tell you how many times I've had to force close FB because a videos audio would stop playing.\n",
      "I've deleted the cache, cleared the data, I've even deleted and re installed the Facebook app,I'm just not able to use the app.\n",
      "It's an amazing app I spend most of my day on it I can see were my family, friends are located I can text s nd pictures and take pictures with beauty filters and I can get many friends add new people from around the country .\n",
      "I really dislike that it insists in changing orientation from landscape to portrait to post, which is particularly bad if you want to use a keyboard cover.\n",
      "Can't hide stories and they take up half the screen.\n",
      "I need to look at \"all\", and then scroll through the entire year to see if there are any birthdays on the actual day.\n",
      "It is such a fun app you can send group messages and you can search your contacts to see who to follow.\n",
      "I reported the problem 2 weeks ago and nothing has been done.\n",
      "When you do find what you were looking for the whole marketplace crashes forcing you to go back and start over.\n",
      "But since a few weeks, when i want to post something on my snapchat story, a picture from my gallery it always moves the picture by it self.\n",
      "I go out of the app, not closing the task, go into notepad, highlight to copy the title, go back into facebook listing, where I left off and everything is gone and removed to start all over again.\n",
      "The latest update has completely ruined large group conversations.\n",
      "Messenger doesnt even pop up at times and I have to go to the app to see if i have a message.\n",
      "It will show snaps as opened when I haven't opened them yet.\n",
      "And (2), why can't they put the one filter I use at the *beginning* of the filter list?\n",
      "No point in having an app that cant even play the song Im looking for.\n",
      "Has stopped updating/refreshing my news feed, I have Internet, I've deleted and reinstalled it, signed out and back in etc...\n",
      "I haven't had any trouble with the site since I first joined but I had to start a new account cuz I forgot my password to my original and first site of social media and I really wanted to stay there but couldn't retrieve it, didn't know how and I had some old school pics of me in early bands that I wanted and info I needed, is there any way I can get my old page back?\n",
      "Why can't I see previous songs instead of just upcoming?\n",
      "- Videos and GIFs randomly change quality and there is no way to select your preferred quality (if there is, you can't change it quickly) - Sometimes getting into and exiting out of a post takes you to a solid black screen for a good few seconds before it snaps out of it.\n",
      "There should be a spot specifically on your page that says Photos where u can click and go in and just view your photos!!\n",
      "Originally it was doing perfectly fine the app works the way it should and everything, But up until recently the app goes dark while scrolling through either of the 2 main pages.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "shuffled_df = topic_modeling_df.sample(frac=1).reset_index(drop=True)\n",
    "while i < 100:\n",
    "    try:\n",
    "        i += 1\n",
    "        print(shuffled_df['review_body'][i])\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=clean_review).fit(topic_modeling_df['review_body'])\n",
    "id2word = dict((v, k) for k, v in bow_transformer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LdaModel in module gensim.models.ldamodel object:\n",
      "\n",
      "class LdaModel(gensim.interfaces.TransformationABC, gensim.models.basemodel.BaseTopicModel)\n",
      " |  LdaModel(corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None, dtype=<class 'numpy.float32'>)\n",
      " |  \n",
      " |  Train and use Online Latent Dirichlet Allocation (OLDA) models as presented in\n",
      " |  `Hoffman et al. :\"Online Learning for Latent Dirichlet Allocation\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |  \n",
      " |  Examples\n",
      " |  -------\n",
      " |  Initialize a model using a Gensim corpus\n",
      " |  \n",
      " |  .. sourcecode:: pycon\n",
      " |  \n",
      " |      >>> from gensim.test.utils import common_corpus\n",
      " |      >>>\n",
      " |      >>> lda = LdaModel(common_corpus, num_topics=10)\n",
      " |  \n",
      " |  You can then infer topic distributions on new, unseen documents.\n",
      " |  \n",
      " |  .. sourcecode:: pycon\n",
      " |  \n",
      " |      >>> doc_bow = [(1, 0.3), (2, 0.1), (0, 0.09)]\n",
      " |      >>> doc_lda = lda[doc_bow]\n",
      " |  \n",
      " |  The model can be updated (trained) with new documents.\n",
      " |  \n",
      " |  .. sourcecode:: pycon\n",
      " |  \n",
      " |      >>> # In practice (corpus =/= initial training corpus), but we use the same here for simplicity.\n",
      " |      >>> other_corpus = common_corpus\n",
      " |      >>>\n",
      " |      >>> lda.update(other_corpus)\n",
      " |  \n",
      " |  Model persistency is achieved through :meth:`~gensim.models.ldamodel.LdaModel.load` and\n",
      " |  :meth:`~gensim.models.ldamodel.LdaModel.save` methods.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaModel\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      gensim.models.basemodel.BaseTopicModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, bow, eps=None)\n",
      " |      Get the topic distribution for the given document.\n",
      " |      \n",
      " |      Wraps :meth:`~gensim.models.ldamodel.LdaModel.get_document_topics` to support an operator style call.\n",
      " |      Uses the model's current state (set using constructor arguments) to fill in the additional arguments of the\n",
      " |      wrapper method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ---------\n",
      " |      bow : list of (int, float)\n",
      " |          The document in BOW format.\n",
      " |      eps : float, optional\n",
      " |          Topics with an assigned probability lower than this threshold will be discarded.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Topic distribution for the given document. Each topic is represented as a pair of its ID and the probability\n",
      " |          assigned to it.\n",
      " |  \n",
      " |  __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None, dtype=<class 'numpy.float32'>)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`).\n",
      " |          If not given, the model is left untrained (presumably because you want to call\n",
      " |          :meth:`~gensim.models.ldamodel.LdaModel.update` manually).\n",
      " |      num_topics : int, optional\n",
      " |          The number of requested latent topics to be extracted from the training corpus.\n",
      " |      id2word : {dict of (int, str), :class:`gensim.corpora.dictionary.Dictionary`}\n",
      " |          Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\n",
      " |          debugging and topic printing.\n",
      " |      distributed : bool, optional\n",
      " |          Whether distributed computing should be used to accelerate training.\n",
      " |      chunksize :  int, optional\n",
      " |          Number of documents to be used in each training chunk.\n",
      " |      passes : int, optional\n",
      " |          Number of passes through the corpus during training.\n",
      " |      update_every : int, optional\n",
      " |          Number of documents to be iterated through for each update.\n",
      " |          Set to 0 for batch learning, > 1 for online iterative learning.\n",
      " |      alpha : {numpy.ndarray, str}, optional\n",
      " |          Can be set to an 1D array of length equal to the number of expected topics that expresses\n",
      " |          our a-priori belief for the each topics' probability.\n",
      " |          Alternatively default prior selecting strategies can be employed by supplying a string:\n",
      " |      \n",
      " |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / topicno`.\n",
      " |              * 'auto': Learns an asymmetric prior from the corpus (not available if `distributed==True`).\n",
      " |      eta : {float, np.array, str}, optional\n",
      " |          A-priori belief on word probability, this can be:\n",
      " |      \n",
      " |              * scalar for a symmetric prior over topic/word probability,\n",
      " |              * vector of length num_words to denote an asymmetric user defined probability for each word,\n",
      " |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
      " |              * the string 'auto' to learn the asymmetric prior from the data.\n",
      " |      decay : float, optional\n",
      " |          A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n",
      " |          when each new document is examined. Corresponds to Kappa from\n",
      " |          `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      offset : float, optional\n",
      " |          Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n",
      " |          Corresponds to Tau_0 from `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      eval_every : int, optional\n",
      " |          Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
      " |      iterations : int, optional\n",
      " |          Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
      " |      gamma_threshold : float, optional\n",
      " |          Minimum change in the value of the gamma parameters to continue iterating.\n",
      " |      minimum_probability : float, optional\n",
      " |          Topics with a probability lower than this threshold will be filtered out.\n",
      " |      random_state : {np.random.RandomState, int}, optional\n",
      " |          Either a randomState object or a seed to generate one. Useful for reproducibility.\n",
      " |      ns_conf : dict of (str, object), optional\n",
      " |          Key word parameters propagated to :func:`gensim.utils.getNS` to get a Pyro4 Nameserved.\n",
      " |          Only used if `distributed` is set to True.\n",
      " |      minimum_phi_value : float, optional\n",
      " |          if `per_word_topics` is True, this represents a lower bound on the term probabilities.\n",
      " |      per_word_topics : bool\n",
      " |          If True, the model also computes a list of topics, sorted in descending order of most likely topics for\n",
      " |          each word, along with their phi values multiplied by the feature length (i.e. word count).\n",
      " |      callbacks : list of :class:`~gensim.models.callbacks.Callback`\n",
      " |          Metric callbacks to log and visualize evaluation metrics of the model during training.\n",
      " |      dtype : {numpy.float16, numpy.float32, numpy.float64}, optional\n",
      " |          Data-type to use during calculations inside model. All inputs are also converted.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Get a string representation of the current object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the most important model parameters.\n",
      " |  \n",
      " |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      " |      Estimate the variational bound of documents from the corpus as E_q[log p(corpus)] - E_q[log q(corpus)].\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`) used to estimate the\n",
      " |          variational bounds.\n",
      " |      gamma : numpy.ndarray, optional\n",
      " |          Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\n",
      " |      subsample_ratio : float, optional\n",
      " |          Percentage of the whole corpus represented by the passed `corpus` argument (in case this was a sample).\n",
      " |          Set to 1.0 if the whole corpus was passed.This is used as a multiplicative factor to scale the likelihood\n",
      " |          appropriately.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The variational bound score calculated for each document.\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear the model's state to free some memory. Used in the distributed implementation.\n",
      " |  \n",
      " |  diff(self, other, distance='kullback_leibler', num_words=100, n_ann_terms=10, diagonal=False, annotation=True, normed=True)\n",
      " |      Calculate the difference in topic distributions between two models: `self` and `other`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`~gensim.models.ldamodel.LdaModel`\n",
      " |          The model which will be compared against the current object.\n",
      " |      distance : {'kullback_leibler', 'hellinger', 'jaccard', 'jensen_shannon'}\n",
      " |          The distance metric to calculate the difference with.\n",
      " |      num_words : int, optional\n",
      " |          The number of most relevant words used if `distance == 'jaccard'`. Also used for annotating topics.\n",
      " |      n_ann_terms : int, optional\n",
      " |          Max number of words in intersection/symmetric difference between topics. Used for annotation.\n",
      " |      diagonal : bool, optional\n",
      " |          Whether we need the difference between identical topics (the diagonal of the difference matrix).\n",
      " |      annotation : bool, optional\n",
      " |          Whether the intersection or difference of words between two topics should be returned.\n",
      " |      normed : bool, optional\n",
      " |          Whether the matrix should be normalized or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          A difference matrix. Each element corresponds to the difference between the two topics,\n",
      " |          shape (`self.num_topics`, `other.num_topics`)\n",
      " |      numpy.ndarray, optional\n",
      " |          Annotation matrix where for each pair we include the word from the intersection of the two topics,\n",
      " |          and the word from the symmetric difference of the two topics. Only included if `annotation == True`.\n",
      " |          Shape (`self.num_topics`, `other_model.num_topics`, 2).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Get the differences between each pair of topics inferred by two models\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models.ldamulticore import LdaMulticore\n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>>\n",
      " |          >>> m1 = LdaMulticore.load(datapath(\"lda_3_0_1_model\"))\n",
      " |          >>> m2 = LdaMulticore.load(datapath(\"ldamodel_python_3_5\"))\n",
      " |          >>> mdiff, annotation = m1.diff(m2)\n",
      " |          >>> topic_diff = mdiff  # get matrix with difference for each topic pair from `m1` and `m2`\n",
      " |  \n",
      " |  do_estep(self, chunk, state=None)\n",
      " |      Perform inference on a chunk of documents, and accumulate the collected sufficient statistics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      state : :class:`~gensim.models.ldamodel.LdaState`, optional\n",
      " |          The state to be updated with the newly accumulated sufficient statistics. If none, the models\n",
      " |          `self.state` is updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Gamma parameters controlling the topic weights, shape (`len(chunk)`, `self.num_topics`).\n",
      " |  \n",
      " |  do_mstep(self, rho, other, extra_pass=False)\n",
      " |      Maximization step: use linear interpolation between the existing topics and\n",
      " |      collected sufficient statistics in `other` to update the topics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      other : :class:`~gensim.models.ldamodel.LdaModel`\n",
      " |          The model whose sufficient statistics will be used to update the topics.\n",
      " |      extra_pass : bool, optional\n",
      " |          Whether this step required an additional pass over the corpus.\n",
      " |  \n",
      " |  get_document_topics(self, bow, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
      " |      Get the topic distribution for the given document.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bow : corpus : list of (int, float)\n",
      " |          The document in BOW format.\n",
      " |      minimum_probability : float\n",
      " |          Topics with an assigned probability lower than this threshold will be discarded.\n",
      " |      minimum_phi_value : float\n",
      " |          If `per_word_topics` is True, this represents a lower bound on the term probabilities that are included.\n",
      " |           If set to None, a value of 1e-8 is used to prevent 0s.\n",
      " |      per_word_topics : bool\n",
      " |          If True, this function will also return two extra lists as explained in the \"Returns\" section.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Topic distribution for the whole document. Each element in the list is a pair of a topic's id, and\n",
      " |          the probability that was assigned to it.\n",
      " |      list of (int, list of (int, float), optional\n",
      " |          Most probable topics per word. Each element in the list is a pair of a word's id, and a list of\n",
      " |          topics sorted by their relevance to this word. Only returned if `per_word_topics` was set to True.\n",
      " |      list of (int, list of float), optional\n",
      " |          Phi relevance values, multiplied by the feature length, for each word-topic combination.\n",
      " |          Each element in the list is a pair of a word's id and a list of the phi values between this word and\n",
      " |          each topic. Only returned if `per_word_topics` was set to True.\n",
      " |  \n",
      " |  get_term_topics(self, word_id, minimum_probability=None)\n",
      " |      Get the most relevant topics to the given word.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_id : int\n",
      " |          The word for which the topic distribution will be computed.\n",
      " |      minimum_probability : float, optional\n",
      " |          Topics with an assigned probability below this threshold will be discarded.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          The relevant topics represented as pairs of their ID and their assigned probability, sorted\n",
      " |          by relevance to the given word.\n",
      " |  \n",
      " |  get_topic_terms(self, topicid, topn=10)\n",
      " |      Get the representation for a single topic. Words the integer IDs, in constrast to\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.show_topic` that represents words by the actual strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicid : int\n",
      " |          The ID of the topic to be returned\n",
      " |      topn : int, optional\n",
      " |          Number of the most significant words that are associated with the topic.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Word ID - probability pairs for the most relevant words generated by the topic.\n",
      " |  \n",
      " |  get_topics(self)\n",
      " |      Get the term-topic matrix learned during inference.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The probability for each word in each topic, shape (`num_topics`, `vocabulary_size`).\n",
      " |  \n",
      " |  inference(self, chunk, collect_sstats=False)\n",
      " |      Given a chunk of sparse document vectors, estimate gamma (parameters controlling the topic weights)\n",
      " |      for each document in the chunk.\n",
      " |      \n",
      " |      This function does not modify the model The whole input chunk of document is assumed to fit in RAM;\n",
      " |      chunking of a large corpus must be done earlier in the pipeline. Avoids computing the `phi` variational\n",
      " |      parameter directly using the optimization presented in\n",
      " |      `Lee, Seung: Algorithms for non-negative matrix factorization\"\n",
      " |      <https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      collect_sstats : bool, optional\n",
      " |          If set to True, also collect (and return) sufficient statistics needed to update the model's topic-word\n",
      " |          distributions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      (numpy.ndarray, {numpy.ndarray, None})\n",
      " |          The first element is always returned and it corresponds to the states gamma matrix. The second element is\n",
      " |          only returned if `collect_sstats` == True and corresponds to the sufficient statistics for the M step.\n",
      " |  \n",
      " |  init_dir_prior(self, prior, name)\n",
      " |      Initialize priors for the Dirichlet distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prior : {str, list of float, numpy.ndarray of float, float}\n",
      " |          A-priori belief on word probability. If `name` == 'eta' then the prior can be:\n",
      " |      \n",
      " |              * scalar for a symmetric prior over topic/word probability,\n",
      " |              * vector of length num_words to denote an asymmetric user defined probability for each word,\n",
      " |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
      " |              * the string 'auto' to learn the asymmetric prior from the data.\n",
      " |      \n",
      " |          If `name` == 'alpha', then the prior can be:\n",
      " |      \n",
      " |              * an 1D array of length equal to the number of expected topics,\n",
      " |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / topicno`.\n",
      " |              * 'auto': Learns an asymmetric prior from the corpus.\n",
      " |      name : {'alpha', 'eta'}\n",
      " |          Whether the `prior` is parameterized by the alpha vector (1 parameter per topic)\n",
      " |          or by the eta (1 parameter per unique term in the vocabulary).\n",
      " |  \n",
      " |  log_perplexity(self, chunk, total_docs=None)\n",
      " |      Calculate and return per-word likelihood bound, using a chunk of documents as evaluation corpus.\n",
      " |      \n",
      " |      Also output the calculated statistics, including the perplexity=2^(-bound), to log at INFO level.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      total_docs : int, optional\n",
      " |          Number of docs used for evaluation of the perplexity.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The variational bound score calculated for each word.\n",
      " |  \n",
      " |  save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you intend to use models across Python 2/3 versions there are a few things to\n",
      " |      keep in mind:\n",
      " |      \n",
      " |        1. The pickled Python dictionaries will not work across Python versions\n",
      " |        2. The `save` method does not automatically save all numpy arrays separately, only\n",
      " |           those ones that exceed `sep_limit` set in :meth:`~gensim.utils.SaveLoad.save`. The main\n",
      " |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      " |      \n",
      " |      Please refer to the `wiki recipes section\n",
      " |      <https://github.com/RaRe-Technologies/gensim/wiki/\n",
      " |      Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2>`_\n",
      " |      for an example on how to work around these issues.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.load`\n",
      " |          Load model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the system file where the model will be persisted.\n",
      " |      ignore : tuple of str, optional\n",
      " |          The named attributes in the tuple will be left out of the pickled model. The reason why\n",
      " |          the internal `state` is ignored by default is that it uses its own serialisation rather than the one\n",
      " |          provided by this method.\n",
      " |      separately : {list of str, None}, optional\n",
      " |          If None -  automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This avoids pickle memory errors and allows `mmap`'ing large arrays\n",
      " |          back on load efficiently. If list of str - this attributes will be stored in separate files,\n",
      " |          the automatic check is not performed in this case.\n",
      " |      *args\n",
      " |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      " |      **kwargs\n",
      " |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10)\n",
      " |      Get the representation for a single topic. Words here are the actual strings, in constrast to\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.get_topic_terms` that represents words by their vocabulary ID.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicid : int\n",
      " |          The ID of the topic to be returned\n",
      " |      topn : int, optional\n",
      " |          Number of the most significant words that are associated with the topic.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          Word - probability pairs for the most relevant words generated by the topic.\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      Get a representation for selected topics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          Number of topics to be returned. Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      " |          The returned topics subset of all topics is therefore arbitrary and may change between two LDA\n",
      " |          training runs.\n",
      " |      num_words : int, optional\n",
      " |          Number of words to be presented for each topic. These will be the most relevant words (assigned the highest\n",
      " |          probability for each topic).\n",
      " |      log : bool, optional\n",
      " |          Whether the output is also logged, besides being returned.\n",
      " |      formatted : bool, optional\n",
      " |          Whether the topic representations should be formatted as strings. If False, they are returned as\n",
      " |          2 tuples of (word, probability).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of {str, tuple of (str, float)}\n",
      " |          a list of topics, each represented either as a string (when `formatted` == True) or word-probability\n",
      " |          pairs.\n",
      " |  \n",
      " |  sync_state(self, current_Elogbeta=None)\n",
      " |      Propagate the states topic probabilities to the inner object's attribute.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      current_Elogbeta: numpy.ndarray\n",
      " |          Posterior probabilities for each topic, optional.\n",
      " |          If omitted, it will get Elogbeta from state.\n",
      " |  \n",
      " |  top_topics(self, corpus=None, texts=None, dictionary=None, window_size=None, coherence='u_mass', topn=20, processes=-1)\n",
      " |      Get the topics with the highest coherence score the coherence for each topic.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of list of (int, float), optional\n",
      " |          Corpus in BoW format.\n",
      " |      texts : list of list of str, optional\n",
      " |          Tokenized texts, needed for coherence models that use sliding window based (i.e. coherence=`c_something`)\n",
      " |          probability estimator .\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n",
      " |          Gensim dictionary mapping of id word to create corpus.\n",
      " |          If `model.id2word` is present, this is not needed. If both are provided, passed `dictionary` will be used.\n",
      " |      window_size : int, optional\n",
      " |          Is the size of the window to be used for coherence measures using boolean sliding window as their\n",
      " |          probability estimator. For 'u_mass' this doesn't matter.\n",
      " |          If None - the default window sizes are used which are: 'c_v' - 110, 'c_uci' - 10, 'c_npmi' - 10.\n",
      " |      coherence : {'u_mass', 'c_v', 'c_uci', 'c_npmi'}, optional\n",
      " |          Coherence measure to be used.\n",
      " |          Fastest method - 'u_mass', 'c_uci' also known as `c_pmi`.\n",
      " |          For 'u_mass' corpus should be provided, if texts is provided, it will be converted to corpus\n",
      " |          using the dictionary. For 'c_v', 'c_uci' and 'c_npmi' `texts` should be provided (`corpus` isn't needed)\n",
      " |      topn : int, optional\n",
      " |          Integer corresponding to the number of top words to be extracted from each topic.\n",
      " |      processes : int, optional\n",
      " |          Number of processes to use for probability estimation phase, any value less than 1 will be interpreted as\n",
      " |          num_cpus - 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (list of (int, str), float)\n",
      " |          Each element in the list is a pair of a topic representation and its coherence score. Topic representations\n",
      " |          are distributions of words, represented as a list of pairs of word IDs and their probabilities.\n",
      " |  \n",
      " |  update(self, corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None, chunks_as_numpy=False)\n",
      " |      Train the model with new documents, by EM-iterating over the corpus until the topics converge, or until\n",
      " |      the maximum number of allowed iterations is reached. `corpus` must be an iterable.\n",
      " |      \n",
      " |      In distributed mode, the E step is distributed over a cluster of machines.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This update also supports updating an already trained model with new documents; the two models are then merged\n",
      " |      in proportion to the number of old vs. new documents. This feature is still experimental for non-stationary\n",
      " |      input streams. For stationary input (no topic drift in new documents), on the other hand, this equals the\n",
      " |      online update of `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |      \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      and is guaranteed to converge for any `decay` in (0.5, 1.0). Additionally, for smaller corpus sizes, an\n",
      " |      increasing `offset` may be beneficial (see Table 1 in the same paper).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`) used to update the\n",
      " |          model.\n",
      " |      chunksize :  int, optional\n",
      " |          Number of documents to be used in each training chunk.\n",
      " |      decay : float, optional\n",
      " |          A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n",
      " |          when each new document is examined. Corresponds to Kappa from\n",
      " |          `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      offset : float, optional\n",
      " |          Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n",
      " |          Corresponds to Tau_0 from `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      passes : int, optional\n",
      " |          Number of passes through the corpus during training.\n",
      " |      update_every : int, optional\n",
      " |          Number of documents to be iterated through for each update.\n",
      " |          Set to 0 for batch learning, > 1 for online iterative learning.\n",
      " |      eval_every : int, optional\n",
      " |          Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
      " |      iterations : int, optional\n",
      " |          Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
      " |      gamma_threshold : float, optional\n",
      " |          Minimum change in the value of the gamma parameters to continue iterating.\n",
      " |      chunks_as_numpy : bool, optional\n",
      " |          Whether each chunk passed to the inference step should be a numpy.ndarray or not. Numpy can in some settings\n",
      " |          turn the term IDs into floats, these will be converted back into integers in inference, which incurs a\n",
      " |          performance hit. For distributed computing it may be desirable to keep the chunks as `numpy.ndarray`.\n",
      " |  \n",
      " |  update_alpha(self, gammat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-document topic weights.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      gammat : numpy.ndarray\n",
      " |          Previous topic weight parameters.\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Sequence of alpha parameters.\n",
      " |  \n",
      " |  update_eta(self, lambdat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-topic word weights.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      lambdat : numpy.ndarray\n",
      " |          Previous lambda parameters.\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The updated eta parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname, *args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`gensim.models.ldamodel.LdaModel` from file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file where the model is stored.\n",
      " |      *args\n",
      " |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n",
      " |      **kwargs\n",
      " |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>>\n",
      " |          >>> fname = datapath(\"lda_3_0_1_model\")\n",
      " |          >>> lda = LdaModel.load(fname, mmap='r')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.basemodel.BaseTopicModel:\n",
      " |  \n",
      " |  print_topic(self, topicno, topn=10)\n",
      " |      Get a single topic as a formatted string.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicno : int\n",
      " |          Topic id.\n",
      " |      topn : int\n",
      " |          Number of words from topic that will be used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          String representation of topic, like '-0.340 * \"category\" + 0.298 * \"$M$\" + 0.183 * \"algebra\" + ... '.\n",
      " |  \n",
      " |  print_topics(self, num_topics=20, num_words=10)\n",
      " |      Get the most significant topics (alias for `show_topics()` method).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          The number of topics to be selected, if -1 - all topics will be in result (ordered by significance).\n",
      " |      num_words : int, optional\n",
      " |          The number of words to be included per topics (ordered by significance).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, list of (str, float))\n",
      " |          Sequence with (topic_id, [(word, value), ... ]).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=7, passes=20)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LdaModel in module gensim.models.ldamodel object:\n",
      "\n",
      "class LdaModel(gensim.interfaces.TransformationABC, gensim.models.basemodel.BaseTopicModel)\n",
      " |  LdaModel(corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None, dtype=<class 'numpy.float32'>)\n",
      " |  \n",
      " |  Train and use Online Latent Dirichlet Allocation (OLDA) models as presented in\n",
      " |  `Hoffman et al. :\"Online Learning for Latent Dirichlet Allocation\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |  \n",
      " |  Examples\n",
      " |  -------\n",
      " |  Initialize a model using a Gensim corpus\n",
      " |  \n",
      " |  .. sourcecode:: pycon\n",
      " |  \n",
      " |      >>> from gensim.test.utils import common_corpus\n",
      " |      >>>\n",
      " |      >>> lda = LdaModel(common_corpus, num_topics=10)\n",
      " |  \n",
      " |  You can then infer topic distributions on new, unseen documents.\n",
      " |  \n",
      " |  .. sourcecode:: pycon\n",
      " |  \n",
      " |      >>> doc_bow = [(1, 0.3), (2, 0.1), (0, 0.09)]\n",
      " |      >>> doc_lda = lda[doc_bow]\n",
      " |  \n",
      " |  The model can be updated (trained) with new documents.\n",
      " |  \n",
      " |  .. sourcecode:: pycon\n",
      " |  \n",
      " |      >>> # In practice (corpus =/= initial training corpus), but we use the same here for simplicity.\n",
      " |      >>> other_corpus = common_corpus\n",
      " |      >>>\n",
      " |      >>> lda.update(other_corpus)\n",
      " |  \n",
      " |  Model persistency is achieved through :meth:`~gensim.models.ldamodel.LdaModel.load` and\n",
      " |  :meth:`~gensim.models.ldamodel.LdaModel.save` methods.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaModel\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      gensim.models.basemodel.BaseTopicModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, bow, eps=None)\n",
      " |      Get the topic distribution for the given document.\n",
      " |      \n",
      " |      Wraps :meth:`~gensim.models.ldamodel.LdaModel.get_document_topics` to support an operator style call.\n",
      " |      Uses the model's current state (set using constructor arguments) to fill in the additional arguments of the\n",
      " |      wrapper method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ---------\n",
      " |      bow : list of (int, float)\n",
      " |          The document in BOW format.\n",
      " |      eps : float, optional\n",
      " |          Topics with an assigned probability lower than this threshold will be discarded.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Topic distribution for the given document. Each topic is represented as a pair of its ID and the probability\n",
      " |          assigned to it.\n",
      " |  \n",
      " |  __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None, dtype=<class 'numpy.float32'>)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`).\n",
      " |          If not given, the model is left untrained (presumably because you want to call\n",
      " |          :meth:`~gensim.models.ldamodel.LdaModel.update` manually).\n",
      " |      num_topics : int, optional\n",
      " |          The number of requested latent topics to be extracted from the training corpus.\n",
      " |      id2word : {dict of (int, str), :class:`gensim.corpora.dictionary.Dictionary`}\n",
      " |          Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\n",
      " |          debugging and topic printing.\n",
      " |      distributed : bool, optional\n",
      " |          Whether distributed computing should be used to accelerate training.\n",
      " |      chunksize :  int, optional\n",
      " |          Number of documents to be used in each training chunk.\n",
      " |      passes : int, optional\n",
      " |          Number of passes through the corpus during training.\n",
      " |      update_every : int, optional\n",
      " |          Number of documents to be iterated through for each update.\n",
      " |          Set to 0 for batch learning, > 1 for online iterative learning.\n",
      " |      alpha : {numpy.ndarray, str}, optional\n",
      " |          Can be set to an 1D array of length equal to the number of expected topics that expresses\n",
      " |          our a-priori belief for the each topics' probability.\n",
      " |          Alternatively default prior selecting strategies can be employed by supplying a string:\n",
      " |      \n",
      " |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / topicno`.\n",
      " |              * 'auto': Learns an asymmetric prior from the corpus (not available if `distributed==True`).\n",
      " |      eta : {float, np.array, str}, optional\n",
      " |          A-priori belief on word probability, this can be:\n",
      " |      \n",
      " |              * scalar for a symmetric prior over topic/word probability,\n",
      " |              * vector of length num_words to denote an asymmetric user defined probability for each word,\n",
      " |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
      " |              * the string 'auto' to learn the asymmetric prior from the data.\n",
      " |      decay : float, optional\n",
      " |          A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n",
      " |          when each new document is examined. Corresponds to Kappa from\n",
      " |          `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      offset : float, optional\n",
      " |          Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n",
      " |          Corresponds to Tau_0 from `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      eval_every : int, optional\n",
      " |          Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
      " |      iterations : int, optional\n",
      " |          Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
      " |      gamma_threshold : float, optional\n",
      " |          Minimum change in the value of the gamma parameters to continue iterating.\n",
      " |      minimum_probability : float, optional\n",
      " |          Topics with a probability lower than this threshold will be filtered out.\n",
      " |      random_state : {np.random.RandomState, int}, optional\n",
      " |          Either a randomState object or a seed to generate one. Useful for reproducibility.\n",
      " |      ns_conf : dict of (str, object), optional\n",
      " |          Key word parameters propagated to :func:`gensim.utils.getNS` to get a Pyro4 Nameserved.\n",
      " |          Only used if `distributed` is set to True.\n",
      " |      minimum_phi_value : float, optional\n",
      " |          if `per_word_topics` is True, this represents a lower bound on the term probabilities.\n",
      " |      per_word_topics : bool\n",
      " |          If True, the model also computes a list of topics, sorted in descending order of most likely topics for\n",
      " |          each word, along with their phi values multiplied by the feature length (i.e. word count).\n",
      " |      callbacks : list of :class:`~gensim.models.callbacks.Callback`\n",
      " |          Metric callbacks to log and visualize evaluation metrics of the model during training.\n",
      " |      dtype : {numpy.float16, numpy.float32, numpy.float64}, optional\n",
      " |          Data-type to use during calculations inside model. All inputs are also converted.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Get a string representation of the current object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the most important model parameters.\n",
      " |  \n",
      " |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      " |      Estimate the variational bound of documents from the corpus as E_q[log p(corpus)] - E_q[log q(corpus)].\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`) used to estimate the\n",
      " |          variational bounds.\n",
      " |      gamma : numpy.ndarray, optional\n",
      " |          Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\n",
      " |      subsample_ratio : float, optional\n",
      " |          Percentage of the whole corpus represented by the passed `corpus` argument (in case this was a sample).\n",
      " |          Set to 1.0 if the whole corpus was passed.This is used as a multiplicative factor to scale the likelihood\n",
      " |          appropriately.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The variational bound score calculated for each document.\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear the model's state to free some memory. Used in the distributed implementation.\n",
      " |  \n",
      " |  diff(self, other, distance='kullback_leibler', num_words=100, n_ann_terms=10, diagonal=False, annotation=True, normed=True)\n",
      " |      Calculate the difference in topic distributions between two models: `self` and `other`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`~gensim.models.ldamodel.LdaModel`\n",
      " |          The model which will be compared against the current object.\n",
      " |      distance : {'kullback_leibler', 'hellinger', 'jaccard', 'jensen_shannon'}\n",
      " |          The distance metric to calculate the difference with.\n",
      " |      num_words : int, optional\n",
      " |          The number of most relevant words used if `distance == 'jaccard'`. Also used for annotating topics.\n",
      " |      n_ann_terms : int, optional\n",
      " |          Max number of words in intersection/symmetric difference between topics. Used for annotation.\n",
      " |      diagonal : bool, optional\n",
      " |          Whether we need the difference between identical topics (the diagonal of the difference matrix).\n",
      " |      annotation : bool, optional\n",
      " |          Whether the intersection or difference of words between two topics should be returned.\n",
      " |      normed : bool, optional\n",
      " |          Whether the matrix should be normalized or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          A difference matrix. Each element corresponds to the difference between the two topics,\n",
      " |          shape (`self.num_topics`, `other.num_topics`)\n",
      " |      numpy.ndarray, optional\n",
      " |          Annotation matrix where for each pair we include the word from the intersection of the two topics,\n",
      " |          and the word from the symmetric difference of the two topics. Only included if `annotation == True`.\n",
      " |          Shape (`self.num_topics`, `other_model.num_topics`, 2).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Get the differences between each pair of topics inferred by two models\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models.ldamulticore import LdaMulticore\n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>>\n",
      " |          >>> m1 = LdaMulticore.load(datapath(\"lda_3_0_1_model\"))\n",
      " |          >>> m2 = LdaMulticore.load(datapath(\"ldamodel_python_3_5\"))\n",
      " |          >>> mdiff, annotation = m1.diff(m2)\n",
      " |          >>> topic_diff = mdiff  # get matrix with difference for each topic pair from `m1` and `m2`\n",
      " |  \n",
      " |  do_estep(self, chunk, state=None)\n",
      " |      Perform inference on a chunk of documents, and accumulate the collected sufficient statistics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      state : :class:`~gensim.models.ldamodel.LdaState`, optional\n",
      " |          The state to be updated with the newly accumulated sufficient statistics. If none, the models\n",
      " |          `self.state` is updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Gamma parameters controlling the topic weights, shape (`len(chunk)`, `self.num_topics`).\n",
      " |  \n",
      " |  do_mstep(self, rho, other, extra_pass=False)\n",
      " |      Maximization step: use linear interpolation between the existing topics and\n",
      " |      collected sufficient statistics in `other` to update the topics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      other : :class:`~gensim.models.ldamodel.LdaModel`\n",
      " |          The model whose sufficient statistics will be used to update the topics.\n",
      " |      extra_pass : bool, optional\n",
      " |          Whether this step required an additional pass over the corpus.\n",
      " |  \n",
      " |  get_document_topics(self, bow, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
      " |      Get the topic distribution for the given document.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bow : corpus : list of (int, float)\n",
      " |          The document in BOW format.\n",
      " |      minimum_probability : float\n",
      " |          Topics with an assigned probability lower than this threshold will be discarded.\n",
      " |      minimum_phi_value : float\n",
      " |          If `per_word_topics` is True, this represents a lower bound on the term probabilities that are included.\n",
      " |           If set to None, a value of 1e-8 is used to prevent 0s.\n",
      " |      per_word_topics : bool\n",
      " |          If True, this function will also return two extra lists as explained in the \"Returns\" section.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Topic distribution for the whole document. Each element in the list is a pair of a topic's id, and\n",
      " |          the probability that was assigned to it.\n",
      " |      list of (int, list of (int, float), optional\n",
      " |          Most probable topics per word. Each element in the list is a pair of a word's id, and a list of\n",
      " |          topics sorted by their relevance to this word. Only returned if `per_word_topics` was set to True.\n",
      " |      list of (int, list of float), optional\n",
      " |          Phi relevance values, multiplied by the feature length, for each word-topic combination.\n",
      " |          Each element in the list is a pair of a word's id and a list of the phi values between this word and\n",
      " |          each topic. Only returned if `per_word_topics` was set to True.\n",
      " |  \n",
      " |  get_term_topics(self, word_id, minimum_probability=None)\n",
      " |      Get the most relevant topics to the given word.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_id : int\n",
      " |          The word for which the topic distribution will be computed.\n",
      " |      minimum_probability : float, optional\n",
      " |          Topics with an assigned probability below this threshold will be discarded.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          The relevant topics represented as pairs of their ID and their assigned probability, sorted\n",
      " |          by relevance to the given word.\n",
      " |  \n",
      " |  get_topic_terms(self, topicid, topn=10)\n",
      " |      Get the representation for a single topic. Words the integer IDs, in constrast to\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.show_topic` that represents words by the actual strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicid : int\n",
      " |          The ID of the topic to be returned\n",
      " |      topn : int, optional\n",
      " |          Number of the most significant words that are associated with the topic.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Word ID - probability pairs for the most relevant words generated by the topic.\n",
      " |  \n",
      " |  get_topics(self)\n",
      " |      Get the term-topic matrix learned during inference.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The probability for each word in each topic, shape (`num_topics`, `vocabulary_size`).\n",
      " |  \n",
      " |  inference(self, chunk, collect_sstats=False)\n",
      " |      Given a chunk of sparse document vectors, estimate gamma (parameters controlling the topic weights)\n",
      " |      for each document in the chunk.\n",
      " |      \n",
      " |      This function does not modify the model The whole input chunk of document is assumed to fit in RAM;\n",
      " |      chunking of a large corpus must be done earlier in the pipeline. Avoids computing the `phi` variational\n",
      " |      parameter directly using the optimization presented in\n",
      " |      `Lee, Seung: Algorithms for non-negative matrix factorization\"\n",
      " |      <https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      collect_sstats : bool, optional\n",
      " |          If set to True, also collect (and return) sufficient statistics needed to update the model's topic-word\n",
      " |          distributions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      (numpy.ndarray, {numpy.ndarray, None})\n",
      " |          The first element is always returned and it corresponds to the states gamma matrix. The second element is\n",
      " |          only returned if `collect_sstats` == True and corresponds to the sufficient statistics for the M step.\n",
      " |  \n",
      " |  init_dir_prior(self, prior, name)\n",
      " |      Initialize priors for the Dirichlet distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prior : {str, list of float, numpy.ndarray of float, float}\n",
      " |          A-priori belief on word probability. If `name` == 'eta' then the prior can be:\n",
      " |      \n",
      " |              * scalar for a symmetric prior over topic/word probability,\n",
      " |              * vector of length num_words to denote an asymmetric user defined probability for each word,\n",
      " |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
      " |              * the string 'auto' to learn the asymmetric prior from the data.\n",
      " |      \n",
      " |          If `name` == 'alpha', then the prior can be:\n",
      " |      \n",
      " |              * an 1D array of length equal to the number of expected topics,\n",
      " |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / topicno`.\n",
      " |              * 'auto': Learns an asymmetric prior from the corpus.\n",
      " |      name : {'alpha', 'eta'}\n",
      " |          Whether the `prior` is parameterized by the alpha vector (1 parameter per topic)\n",
      " |          or by the eta (1 parameter per unique term in the vocabulary).\n",
      " |  \n",
      " |  log_perplexity(self, chunk, total_docs=None)\n",
      " |      Calculate and return per-word likelihood bound, using a chunk of documents as evaluation corpus.\n",
      " |      \n",
      " |      Also output the calculated statistics, including the perplexity=2^(-bound), to log at INFO level.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : {list of list of (int, float), scipy.sparse.csc}\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      total_docs : int, optional\n",
      " |          Number of docs used for evaluation of the perplexity.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The variational bound score calculated for each word.\n",
      " |  \n",
      " |  save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you intend to use models across Python 2/3 versions there are a few things to\n",
      " |      keep in mind:\n",
      " |      \n",
      " |        1. The pickled Python dictionaries will not work across Python versions\n",
      " |        2. The `save` method does not automatically save all numpy arrays separately, only\n",
      " |           those ones that exceed `sep_limit` set in :meth:`~gensim.utils.SaveLoad.save`. The main\n",
      " |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      " |      \n",
      " |      Please refer to the `wiki recipes section\n",
      " |      <https://github.com/RaRe-Technologies/gensim/wiki/\n",
      " |      Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2>`_\n",
      " |      for an example on how to work around these issues.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.load`\n",
      " |          Load model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the system file where the model will be persisted.\n",
      " |      ignore : tuple of str, optional\n",
      " |          The named attributes in the tuple will be left out of the pickled model. The reason why\n",
      " |          the internal `state` is ignored by default is that it uses its own serialisation rather than the one\n",
      " |          provided by this method.\n",
      " |      separately : {list of str, None}, optional\n",
      " |          If None -  automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This avoids pickle memory errors and allows `mmap`'ing large arrays\n",
      " |          back on load efficiently. If list of str - this attributes will be stored in separate files,\n",
      " |          the automatic check is not performed in this case.\n",
      " |      *args\n",
      " |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      " |      **kwargs\n",
      " |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10)\n",
      " |      Get the representation for a single topic. Words here are the actual strings, in constrast to\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.get_topic_terms` that represents words by their vocabulary ID.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicid : int\n",
      " |          The ID of the topic to be returned\n",
      " |      topn : int, optional\n",
      " |          Number of the most significant words that are associated with the topic.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          Word - probability pairs for the most relevant words generated by the topic.\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      Get a representation for selected topics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          Number of topics to be returned. Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      " |          The returned topics subset of all topics is therefore arbitrary and may change between two LDA\n",
      " |          training runs.\n",
      " |      num_words : int, optional\n",
      " |          Number of words to be presented for each topic. These will be the most relevant words (assigned the highest\n",
      " |          probability for each topic).\n",
      " |      log : bool, optional\n",
      " |          Whether the output is also logged, besides being returned.\n",
      " |      formatted : bool, optional\n",
      " |          Whether the topic representations should be formatted as strings. If False, they are returned as\n",
      " |          2 tuples of (word, probability).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of {str, tuple of (str, float)}\n",
      " |          a list of topics, each represented either as a string (when `formatted` == True) or word-probability\n",
      " |          pairs.\n",
      " |  \n",
      " |  sync_state(self, current_Elogbeta=None)\n",
      " |      Propagate the states topic probabilities to the inner object's attribute.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      current_Elogbeta: numpy.ndarray\n",
      " |          Posterior probabilities for each topic, optional.\n",
      " |          If omitted, it will get Elogbeta from state.\n",
      " |  \n",
      " |  top_topics(self, corpus=None, texts=None, dictionary=None, window_size=None, coherence='u_mass', topn=20, processes=-1)\n",
      " |      Get the topics with the highest coherence score the coherence for each topic.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of list of (int, float), optional\n",
      " |          Corpus in BoW format.\n",
      " |      texts : list of list of str, optional\n",
      " |          Tokenized texts, needed for coherence models that use sliding window based (i.e. coherence=`c_something`)\n",
      " |          probability estimator .\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n",
      " |          Gensim dictionary mapping of id word to create corpus.\n",
      " |          If `model.id2word` is present, this is not needed. If both are provided, passed `dictionary` will be used.\n",
      " |      window_size : int, optional\n",
      " |          Is the size of the window to be used for coherence measures using boolean sliding window as their\n",
      " |          probability estimator. For 'u_mass' this doesn't matter.\n",
      " |          If None - the default window sizes are used which are: 'c_v' - 110, 'c_uci' - 10, 'c_npmi' - 10.\n",
      " |      coherence : {'u_mass', 'c_v', 'c_uci', 'c_npmi'}, optional\n",
      " |          Coherence measure to be used.\n",
      " |          Fastest method - 'u_mass', 'c_uci' also known as `c_pmi`.\n",
      " |          For 'u_mass' corpus should be provided, if texts is provided, it will be converted to corpus\n",
      " |          using the dictionary. For 'c_v', 'c_uci' and 'c_npmi' `texts` should be provided (`corpus` isn't needed)\n",
      " |      topn : int, optional\n",
      " |          Integer corresponding to the number of top words to be extracted from each topic.\n",
      " |      processes : int, optional\n",
      " |          Number of processes to use for probability estimation phase, any value less than 1 will be interpreted as\n",
      " |          num_cpus - 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (list of (int, str), float)\n",
      " |          Each element in the list is a pair of a topic representation and its coherence score. Topic representations\n",
      " |          are distributions of words, represented as a list of pairs of word IDs and their probabilities.\n",
      " |  \n",
      " |  update(self, corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None, chunks_as_numpy=False)\n",
      " |      Train the model with new documents, by EM-iterating over the corpus until the topics converge, or until\n",
      " |      the maximum number of allowed iterations is reached. `corpus` must be an iterable.\n",
      " |      \n",
      " |      In distributed mode, the E step is distributed over a cluster of machines.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This update also supports updating an already trained model with new documents; the two models are then merged\n",
      " |      in proportion to the number of old vs. new documents. This feature is still experimental for non-stationary\n",
      " |      input streams. For stationary input (no topic drift in new documents), on the other hand, this equals the\n",
      " |      online update of `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |      \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      and is guaranteed to converge for any `decay` in (0.5, 1.0). Additionally, for smaller corpus sizes, an\n",
      " |      increasing `offset` may be beneficial (see Table 1 in the same paper).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_terms`, `num_documents`) used to update the\n",
      " |          model.\n",
      " |      chunksize :  int, optional\n",
      " |          Number of documents to be used in each training chunk.\n",
      " |      decay : float, optional\n",
      " |          A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n",
      " |          when each new document is examined. Corresponds to Kappa from\n",
      " |          `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      offset : float, optional\n",
      " |          Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n",
      " |          Corresponds to Tau_0 from `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
      " |          \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
      " |      passes : int, optional\n",
      " |          Number of passes through the corpus during training.\n",
      " |      update_every : int, optional\n",
      " |          Number of documents to be iterated through for each update.\n",
      " |          Set to 0 for batch learning, > 1 for online iterative learning.\n",
      " |      eval_every : int, optional\n",
      " |          Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
      " |      iterations : int, optional\n",
      " |          Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
      " |      gamma_threshold : float, optional\n",
      " |          Minimum change in the value of the gamma parameters to continue iterating.\n",
      " |      chunks_as_numpy : bool, optional\n",
      " |          Whether each chunk passed to the inference step should be a numpy.ndarray or not. Numpy can in some settings\n",
      " |          turn the term IDs into floats, these will be converted back into integers in inference, which incurs a\n",
      " |          performance hit. For distributed computing it may be desirable to keep the chunks as `numpy.ndarray`.\n",
      " |  \n",
      " |  update_alpha(self, gammat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-document topic weights.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      gammat : numpy.ndarray\n",
      " |          Previous topic weight parameters.\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Sequence of alpha parameters.\n",
      " |  \n",
      " |  update_eta(self, lambdat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-topic word weights.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      lambdat : numpy.ndarray\n",
      " |          Previous lambda parameters.\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The updated eta parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname, *args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`gensim.models.ldamodel.LdaModel` from file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file where the model is stored.\n",
      " |      *args\n",
      " |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n",
      " |      **kwargs\n",
      " |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>>\n",
      " |          >>> fname = datapath(\"lda_3_0_1_model\")\n",
      " |          >>> lda = LdaModel.load(fname, mmap='r')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.basemodel.BaseTopicModel:\n",
      " |  \n",
      " |  print_topic(self, topicno, topn=10)\n",
      " |      Get a single topic as a formatted string.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicno : int\n",
      " |          Topic id.\n",
      " |      topn : int\n",
      " |          Number of words from topic that will be used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          String representation of topic, like '-0.340 * \"category\" + 0.298 * \"$M$\" + 0.183 * \"algebra\" + ... '.\n",
      " |  \n",
      " |  print_topics(self, num_topics=20, num_words=10)\n",
      " |      Get the most significant topics (alias for `show_topics()` method).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          The number of topics to be selected, if -1 - all topics will be in result (ordered by significance).\n",
      " |      num_words : int, optional\n",
      " |          The number of words to be included per topics (ordered by significance).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, list of (str, float))\n",
      " |          Sequence with (topic_id, [(word, value), ... ]).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "pyLDAvis.genesim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "informative_clean = pd.DataFrame(columns=['review'])\n",
    "for i in range(len(json_reviews_by_sent)):\n",
    "    if json_reviews_by_sent['classification'][i] == 'informative':\n",
    "        clean_sentence = []\n",
    "        for w in json_reviews_by_sent['review_body'][i].split():\n",
    "            if w not in stopwords_set:\n",
    "                clean_sentence.append(w)\n",
    "        s = ' '.join(clean_sentence)\n",
    "        informative_clean = informative_clean.append({'review': s}, ignore_index=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nouns = pd.DataFrame(informative_clean['review'].apply(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/qh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a10s</th>\n",
       "      <th>a50</th>\n",
       "      <th>abit</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>accounts</th>\n",
       "      <th>acount</th>\n",
       "      <th>action</th>\n",
       "      <th>activation</th>\n",
       "      <th>activites</th>\n",
       "      <th>...</th>\n",
       "      <th>yadda</th>\n",
       "      <th>yall</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yell</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yolo</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>815 rows  936 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     a10s  a50  abit  access  account  accounts  acount  action  activation  \\\n",
       "0       0    0     0       0        0         0       0       0           0   \n",
       "1       0    0     0       0        0         0       0       0           0   \n",
       "2       0    0     0       0        0         0       0       0           0   \n",
       "3       0    0     0       0        0         0       0       0           0   \n",
       "4       0    0     0       0        1         0       0       0           0   \n",
       "5       0    0     0       0        0         0       0       0           0   \n",
       "6       0    0     0       0        0         0       0       0           0   \n",
       "7       0    0     0       0        0         0       0       0           0   \n",
       "8       0    0     0       0        0         0       0       0           0   \n",
       "9       0    0     0       0        0         0       0       0           0   \n",
       "10      0    0     0       0        0         0       0       0           0   \n",
       "11      0    0     0       0        0         0       0       0           0   \n",
       "12      0    0     0       0        0         0       0       0           0   \n",
       "13      0    0     0       0        0         0       0       0           0   \n",
       "14      0    0     0       0        0         0       0       0           0   \n",
       "15      0    0     0       0        0         0       0       0           0   \n",
       "16      0    0     0       0        0         0       0       0           0   \n",
       "17      0    0     0       0        0         0       0       0           0   \n",
       "18      0    0     0       0        0         0       0       0           0   \n",
       "19      0    0     0       0        0         0       0       0           0   \n",
       "20      0    0     0       0        0         0       0       0           0   \n",
       "21      0    0     0       0        0         0       0       0           0   \n",
       "22      0    0     0       0        0         0       0       0           0   \n",
       "23      0    0     0       0        0         0       0       0           0   \n",
       "24      0    0     0       0        0         0       0       0           0   \n",
       "25      0    0     0       0        0         0       0       0           0   \n",
       "26      0    0     0       0        0         0       0       0           0   \n",
       "27      0    0     0       0        0         0       0       0           0   \n",
       "28      0    0     0       0        0         0       0       0           0   \n",
       "29      0    0     0       0        0         0       0       0           0   \n",
       "..    ...  ...   ...     ...      ...       ...     ...     ...         ...   \n",
       "785     0    0     0       0        0         0       0       0           0   \n",
       "786     0    0     0       0        0         0       0       0           0   \n",
       "787     0    0     0       0        0         0       0       0           0   \n",
       "788     0    0     0       0        1         0       0       0           0   \n",
       "789     0    0     0       0        0         0       0       0           0   \n",
       "790     0    0     0       0        0         0       0       0           0   \n",
       "791     0    0     0       0        0         0       0       0           0   \n",
       "792     0    0     0       0        0         0       0       0           0   \n",
       "793     0    0     0       0        0         0       0       0           0   \n",
       "794     0    0     0       0        0         0       0       0           0   \n",
       "795     0    0     0       0        0         0       0       0           0   \n",
       "796     0    0     0       0        0         0       0       0           0   \n",
       "797     0    0     0       0        0         0       0       0           0   \n",
       "798     0    0     0       0        0         0       0       0           0   \n",
       "799     1    0     0       0        0         0       0       0           0   \n",
       "800     0    0     0       0        0         0       0       0           0   \n",
       "801     0    0     0       0        0         0       0       0           0   \n",
       "802     0    0     0       0        0         0       0       0           0   \n",
       "803     0    0     0       0        0         0       0       0           0   \n",
       "804     0    0     0       0        0         0       0       0           0   \n",
       "805     0    0     0       0        0         0       0       0           0   \n",
       "806     0    0     0       0        0         0       0       0           0   \n",
       "807     0    0     0       0        0         0       0       0           0   \n",
       "808     0    0     0       0        0         0       0       0           0   \n",
       "809     0    0     0       0        0         0       0       0           0   \n",
       "810     0    0     0       0        0         0       0       0           0   \n",
       "811     0    0     0       0        0         0       0       0           0   \n",
       "812     0    0     0       0        0         0       0       0           0   \n",
       "813     0    0     0       0        0         0       0       0           0   \n",
       "814     0    0     0       0        0         0       0       0           0   \n",
       "\n",
       "     activites  ...  yadda  yall  year  years  yell  yesterday  yolo  youtube  \\\n",
       "0            0  ...      0     0     0      0     0          0     0        0   \n",
       "1            0  ...      0     0     0      0     0          0     0        0   \n",
       "2            0  ...      0     0     0      0     0          0     0        0   \n",
       "3            0  ...      0     0     0      0     0          0     0        0   \n",
       "4            0  ...      0     0     0      0     0          0     0        0   \n",
       "5            0  ...      0     0     0      0     0          0     0        0   \n",
       "6            0  ...      0     0     0      0     0          0     0        0   \n",
       "7            0  ...      0     0     0      0     0          0     0        1   \n",
       "8            0  ...      0     0     0      0     0          0     0        0   \n",
       "9            0  ...      0     0     0      0     0          0     0        0   \n",
       "10           0  ...      0     0     0      0     0          0     0        0   \n",
       "11           0  ...      0     0     0      0     0          0     0        0   \n",
       "12           0  ...      0     0     0      0     0          0     0        0   \n",
       "13           0  ...      0     0     0      0     0          0     0        0   \n",
       "14           0  ...      0     0     0      0     0          0     0        0   \n",
       "15           0  ...      0     0     0      1     0          0     0        0   \n",
       "16           0  ...      0     0     0      0     0          0     0        0   \n",
       "17           0  ...      0     0     0      0     0          0     0        0   \n",
       "18           0  ...      0     0     0      0     0          0     0        0   \n",
       "19           0  ...      0     0     0      0     0          0     0        0   \n",
       "20           0  ...      0     0     0      0     0          0     0        0   \n",
       "21           0  ...      0     0     0      0     0          0     0        0   \n",
       "22           0  ...      0     0     0      0     0          0     0        0   \n",
       "23           0  ...      0     0     0      0     0          0     0        0   \n",
       "24           0  ...      0     0     0      0     0          0     0        0   \n",
       "25           0  ...      0     0     0      0     0          0     0        0   \n",
       "26           0  ...      0     0     0      0     0          0     0        0   \n",
       "27           0  ...      0     0     0      0     0          0     0        0   \n",
       "28           0  ...      0     0     0      0     0          0     0        0   \n",
       "29           0  ...      0     0     0      0     0          0     0        0   \n",
       "..         ...  ...    ...   ...   ...    ...   ...        ...   ...      ...   \n",
       "785          0  ...      0     0     0      0     0          0     0        0   \n",
       "786          0  ...      0     0     0      0     0          0     0        0   \n",
       "787          0  ...      0     0     0      0     0          0     0        0   \n",
       "788          0  ...      0     0     0      0     0          0     0        0   \n",
       "789          0  ...      0     0     0      0     0          0     0        0   \n",
       "790          0  ...      0     0     0      0     0          0     0        0   \n",
       "791          0  ...      0     0     0      0     0          0     0        0   \n",
       "792          0  ...      0     0     0      0     0          0     0        0   \n",
       "793          0  ...      0     0     0      0     0          0     0        0   \n",
       "794          0  ...      0     0     0      0     0          0     0        0   \n",
       "795          0  ...      0     0     0      0     0          0     0        0   \n",
       "796          0  ...      0     0     0      0     0          0     0        0   \n",
       "797          0  ...      0     0     0      0     0          0     0        0   \n",
       "798          0  ...      0     0     0      0     0          0     0        0   \n",
       "799          0  ...      0     0     0      0     0          0     0        0   \n",
       "800          0  ...      0     0     0      0     0          0     0        0   \n",
       "801          0  ...      0     0     0      0     0          0     0        0   \n",
       "802          0  ...      0     0     0      0     0          0     0        0   \n",
       "803          0  ...      0     0     0      0     0          0     0        0   \n",
       "804          0  ...      0     0     0      0     0          0     0        0   \n",
       "805          0  ...      0     0     0      0     0          0     0        0   \n",
       "806          0  ...      0     0     0      0     0          0     0        0   \n",
       "807          0  ...      0     0     0      0     0          0     0        0   \n",
       "808          0  ...      0     0     0      0     0          0     0        0   \n",
       "809          0  ...      0     0     0      0     0          0     0        0   \n",
       "810          0  ...      0     0     0      0     0          0     0        0   \n",
       "811          0  ...      0     0     0      0     0          0     0        0   \n",
       "812          0  ...      0     0     0      0     0          0     0        0   \n",
       "813          0  ...      0     0     0      0     0          0     0        0   \n",
       "814          0  ...      0     0     0      0     0          0     0        0   \n",
       "\n",
       "     zoom  zooms  \n",
       "0       0      0  \n",
       "1       0      0  \n",
       "2       0      0  \n",
       "3       0      0  \n",
       "4       0      0  \n",
       "5       0      0  \n",
       "6       0      0  \n",
       "7       0      0  \n",
       "8       0      0  \n",
       "9       0      0  \n",
       "10      0      0  \n",
       "11      0      0  \n",
       "12      0      0  \n",
       "13      0      0  \n",
       "14      0      0  \n",
       "15      0      0  \n",
       "16      0      0  \n",
       "17      0      0  \n",
       "18      0      0  \n",
       "19      0      0  \n",
       "20      0      0  \n",
       "21      0      0  \n",
       "22      0      0  \n",
       "23      0      0  \n",
       "24      0      0  \n",
       "25      0      0  \n",
       "26      0      0  \n",
       "27      0      0  \n",
       "28      0      0  \n",
       "29      1      0  \n",
       "..    ...    ...  \n",
       "785     0      0  \n",
       "786     0      0  \n",
       "787     0      0  \n",
       "788     0      0  \n",
       "789     0      0  \n",
       "790     0      0  \n",
       "791     0      0  \n",
       "792     0      0  \n",
       "793     0      0  \n",
       "794     0      0  \n",
       "795     0      0  \n",
       "796     0      0  \n",
       "797     0      0  \n",
       "798     0      0  \n",
       "799     0      0  \n",
       "800     0      0  \n",
       "801     0      0  \n",
       "802     0      0  \n",
       "803     0      0  \n",
       "804     0      0  \n",
       "805     0      0  \n",
       "806     0      0  \n",
       "807     0      0  \n",
       "808     0      0  \n",
       "809     0      0  \n",
       "810     0      0  \n",
       "811     0      0  \n",
       "812     0      0  \n",
       "813     0      0  \n",
       "814     0      0  \n",
       "\n",
       "[815 rows x 936 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns['review'])\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.075*\"notifications\" + 0.041*\"app\" + 0.017*\"list\" + 0.015*\"phone\" + 0.015*\"feed\" + 0.013*\"ads\" + 0.012*\"snaps\" + 0.012*\"version\" + 0.012*\"work\" + 0.011*\"button\"'),\n",
       " (1,\n",
       "  '0.030*\"friends\" + 0.021*\"screen\" + 0.019*\"page\" + 0.018*\"posts\" + 0.018*\"videos\" + 0.018*\"post\" + 0.015*\"songs\" + 0.013*\"photo\" + 0.013*\"story\" + 0.013*\"play\"'),\n",
       " (2,\n",
       "  '0.051*\"app\" + 0.035*\"notification\" + 0.015*\"music\" + 0.015*\"camera\" + 0.013*\"reason\" + 0.013*\"message\" + 0.012*\"way\" + 0.012*\"pages\" + 0.012*\"posts\" + 0.011*\"users\"'),\n",
       " (3,\n",
       "  '0.027*\"video\" + 0.022*\"facebook\" + 0.020*\"problem\" + 0.019*\"app\" + 0.019*\"phone\" + 0.018*\"times\" + 0.015*\"option\" + 0.013*\"posts\" + 0.013*\"messages\" + 0.013*\"account\"')]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new notifications settings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>okay overall better change thumbnail items cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>app fine voice constant ads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>today latest pictures month November</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Overall app nice someone log account phone Oop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>months</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>order fix reinstalled app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>different YouTube ads right video NEVER EVER i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>able view notifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>way unlock phone bring app starts beginning song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>server cant handle multiple photos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>unusual videos see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Theres couple weeks ridiculous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>used GB mobile data last days background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>thing delete block songs playlist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>years app speak friends star moment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ad app middle something FORCED click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>radius selection tool works radius select</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>updates good features apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cameo problem option help system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>thing Y'all Mark All Read feature LITE version</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>occasional bug messenger notification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>shuffle play unwanted songs MAY song want skips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GB storage % battery last hours minutes use mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Half time add people list friends list % uD83E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>many obscure albums able track right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>fine last week picture camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>video videos newest video video watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bit main screen search name map able click name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mess streaks update android zoom gify hard cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>Third ALL songs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>notification menu old notifications light shad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>interacting posts many images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>famous celebrities viral video plese chance pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>posts pages FB pages post fans posts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>code number old cell number time FB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>data cache phone download app log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>issues list friends N'T HIDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>click notification.. ALSO Messenger notificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>old posts time wall update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>last months trouble notifications snap send ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>watch stories new stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>public pages Facebook comments option switch p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Great app internet works tik tok search someth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>phone Facebook app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>Facebook use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>issue song main search bar playlists search do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>fix security issues anyone worries secure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>review months upload snaps wrong order fixed.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>fan apps colleagues blue Advertising conversat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>internet connection thing phone needs internet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>app open try</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>WiFi fine able logout work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>Facebook work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>way touch people run business fb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>Im moderator page time page samsung phone butt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>notifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>everything app im fond general look changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>inconvenience save post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>815 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review\n",
       "0                           new notifications settings\n",
       "1    okay overall better change thumbnail items cur...\n",
       "2                          app fine voice constant ads\n",
       "3                 today latest pictures month November\n",
       "4    Overall app nice someone log account phone Oop...\n",
       "5                                               months\n",
       "6                            order fix reinstalled app\n",
       "7    different YouTube ads right video NEVER EVER i...\n",
       "8                              able view notifications\n",
       "9     way unlock phone bring app starts beginning song\n",
       "10                  server cant handle multiple photos\n",
       "11                                  unusual videos see\n",
       "12                      Theres couple weeks ridiculous\n",
       "13            used GB mobile data last days background\n",
       "14                   thing delete block songs playlist\n",
       "15                 years app speak friends star moment\n",
       "16                ad app middle something FORCED click\n",
       "17           radius selection tool works radius select\n",
       "18                         updates good features apple\n",
       "19                    cameo problem option help system\n",
       "20      thing Y'all Mark All Read feature LITE version\n",
       "21               occasional bug messenger notification\n",
       "22     shuffle play unwanted songs MAY song want skips\n",
       "23   GB storage % battery last hours minutes use mi...\n",
       "24   Half time add people list friends list % uD83E...\n",
       "25                many obscure albums able track right\n",
       "26                       fine last week picture camera\n",
       "27               video videos newest video video watch\n",
       "28     bit main screen search name map able click name\n",
       "29   mess streaks update android zoom gify hard cre...\n",
       "..                                                 ...\n",
       "785                                    Third ALL songs\n",
       "786  notification menu old notifications light shad...\n",
       "787                      interacting posts many images\n",
       "788  famous celebrities viral video plese chance pe...\n",
       "789               posts pages FB pages post fans posts\n",
       "790                code number old cell number time FB\n",
       "791                  data cache phone download app log\n",
       "792                       issues list friends N'T HIDE\n",
       "793  click notification.. ALSO Messenger notificati...\n",
       "794                         old posts time wall update\n",
       "795  last months trouble notifications snap send ap...\n",
       "796                            watch stories new stuff\n",
       "797  public pages Facebook comments option switch p...\n",
       "798                                               time\n",
       "799  Great app internet works tik tok search someth...\n",
       "800                                 phone Facebook app\n",
       "801                                       Facebook use\n",
       "802  issue song main search bar playlists search do...\n",
       "803          fix security issues anyone worries secure\n",
       "804  review months upload snaps wrong order fixed.....\n",
       "805  fan apps colleagues blue Advertising conversat...\n",
       "806  internet connection thing phone needs internet...\n",
       "807                                       app open try\n",
       "808                         WiFi fine able logout work\n",
       "809                                      Facebook work\n",
       "810                   way touch people run business fb\n",
       "811  Im moderator page time page samsung phone butt...\n",
       "812                                      notifications\n",
       "813        everything app im fond general look changes\n",
       "814                            inconvenience save post\n",
       "\n",
       "[815 rows x 1 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns_adj = pd.DataFrame(informative_clean['review'].apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj['review'])\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "\n",
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"notification\" + 0.016*\"app\" + 0.016*\"snap\" + 0.016*\"messages\" + 0.015*\"message\" + 0.015*\"notifications\" + 0.015*\"screen\" + 0.014*\"videos\" + 0.014*\"snaps\" + 0.013*\"post\"'),\n",
       " (1,\n",
       "  '0.046*\"app\" + 0.016*\"posts\" + 0.014*\"facebook\" + 0.013*\"way\" + 0.011*\"pages\" + 0.011*\"thing\" + 0.010*\"good\" + 0.010*\"video\" + 0.008*\"post\" + 0.008*\"videos\"'),\n",
       " (2,\n",
       "  '0.032*\"app\" + 0.027*\"update\" + 0.024*\"posts\" + 0.018*\"facebook\" + 0.013*\"camera\" + 0.013*\"feed\" + 0.013*\"friends\" + 0.012*\"days\" + 0.011*\"content\" + 0.011*\"version\"'),\n",
       " (3,\n",
       "  '0.039*\"notifications\" + 0.023*\"app\" + 0.020*\"video\" + 0.019*\"account\" + 0.016*\"problem\" + 0.015*\"new\" + 0.015*\"phone\" + 0.014*\"list\" + 0.013*\"friends\" + 0.012*\"comments\"')]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=200)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_nlu.training_data  import load_data\n",
    "from rasa_nlu.config import RasaNLUModelConfig\n",
    "from rasa_nlu.model import Trainer\n",
    "from rasa_nlu import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data('rasa_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config.load('config_spacy.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rasa_nlu.model.Interpreter at 0x14dae7850>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = trainer.persist('./projects/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa_nlu.model import Metadata, Interpreter\n",
    "interpreter = Interpreter.load(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n",
      "{'intent': {'name': None, 'confidence': 0.0}, 'entities': [], 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(informative_clean)):\n",
    "    try:\n",
    "        print(interpreter.parse(text=informative_df['review'][i]))\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/qh/Documents/capstone', '/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python37.zip', '/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7', '/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload', '', '/usr/local/lib/python3.7/site-packages', '/usr/local/lib/python3.7/site-packages/geos', '/usr/local/Cellar/numpy/1.16.0/libexec/nose/lib/python3.7/site-packages', '/usr/local/lib/python3.7/site-packages/IPython/extensions', '/Users/qh/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I start to scroll down it starts showing me videos I've never seen before!\n",
      "Now, after deleting, I'm trying to sign back in to the app, and even that's not working.\n",
      "Though navigation to other things is weird, and when you upvote or downvote when the picture is opened and then close picture it then it's not upvoted anymore, but still a great community.\n",
      "Also, the cover art thumbnails are way too small.\n",
      "Using the internet browser is a better experience than this app.\n",
      "Sometimes duplicated notifications.\n",
      "I would really love it if you could zoom in on stickers more.\n",
      "When I'm reading comments on a video, itll sometimes continue to play the audio on the video which is rather annoying.\n",
      "When i finished with the account identifying, it shows a blank page fill with languages.\n",
      "Facebook has so many problems%uD83D%uDE1E Pictures & videos r not uoload clearly as phone..its bad..it should be clear hd colorful.. And fb should make sure every pictures security not just locked profile pro pics..each n every pics albums we upload should be a choice to make more secure gaurd that if any one wants that nobody screenshot it or save or share it..just add one more option for more privacy and secure.. pls do this hurry up and make sure fb videos and pictures good quality its worse..\n",
      "therefore, Please install #\"dark mode\" to protect our eye With optional, if anyone want to open the mode they could easily do it If your team do it then I'll be very grateful to you and I hope you'll do it as soon as possible Thanks\n",
      "Notifications on a good day are seemingly in no specific order, completely randomized, click one to check on it, then go back to notifications and everything is in a completely different order.\n",
      "Should have integrated the chat app with social media app.\n",
      "It's very glitchy and half of the time I can't access the top of the screen because I can't scroll up all of the way to view it.\n",
      "(1) I absolutely hate the double click to like something feature.\n",
      "When I check my latest feed I can see how much I am actually missing but it is hard to catch up with friends and such as my group posts tend to take over.\n",
      "It was pretty fun, but what i really hate is when i woke up the facebook keeps blocking me and telling me that session expired, that i should create a new account.\n",
      "Worst part is most the time those stories are suggested posts from people that aren't even friends so it's really hard to find them again.\n",
      "The update today won't let anything load.\n",
      "I'm having problems with the first notification being covered by the banner to chose feed, friends, video, profile or notifications across all five tabs.\n",
      "I can't even check what my friends are sending to me.\n",
      "If I continue to scroll I still hear all the sounds the next video would play.\n",
      "I personally would like to be able to schedule meetings and events in real time and have more detailed explanations on how to use every feature that the app has to offer a note; especially for those over 50 that are not as tech-savvy as most people these days, including myself.\n",
      "Both accounts should look the same but they dont!!!!!!\n",
      "I'll get a notification that I got a snap from someone, I open the notification and then the app says I've viewed the message, when I haven't, and then I can't replay it either.\n",
      "Every time I'm watching a video itll back out and refresh the home page.\n",
      "Too many ads, all similar services are advertised to me even after removing things from my information.\n",
      "Slow even with 4g and and a 8GB ram phone.\n",
      "Its awful and pointless and I hate the automatic refreshing that happens for literally no reason.\n",
      "On top of this, it becomes randomly convinced I'm offline, forcing me to completely close out and restart (losing any and all posts I may have made as I get no warning when it will happen).\n"
     ]
    }
   ],
   "source": [
    "informative_df = json_reviews_by_sent[json_reviews_by_sent['classification'] == 'informative']\n",
    "for i in range(400, 500):\n",
    "    try:\n",
    "        print(informative_df['review_body'][i])\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really like this app but alot of things could be fixed.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-7be416b9b3ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_reviews_by_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(100):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
